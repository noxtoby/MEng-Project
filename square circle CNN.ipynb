{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras import backend as K\n",
    "\n",
    "batch_size = 64\n",
    "num_classes = 1\n",
    "epochs = 5\n",
    "\n",
    "# input image dimensions\n",
    "img_rows, img_cols = 70, 70\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:03<00:00, 290.49it/s]\n",
      "100%|██████████| 1000/1000 [00:03<00:00, 284.62it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "import csv\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from skimage.transform import rotate\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "\n",
    "trainxs = []\n",
    "trainximgs = []\n",
    "trainys = []\n",
    "\n",
    "DATADIR = r\"C:\\Users\\colly\\Desktop\\squarevscircle\"\n",
    "\n",
    "CATEGORIES = [\"square\", \"circle\"]\n",
    "#CATEGORIES = [\"out_square\", \"in_square\", \"on_line\"]\n",
    "\n",
    "for category in CATEGORIES:\n",
    "    path = os.path.join(DATADIR,category) \n",
    "    class_num = CATEGORIES.index(category)\n",
    "    for img in os.listdir(path):  \n",
    "        img_array = cv2.imread(os.path.join(path,img) ,cv2.IMREAD_GRAYSCALE)  # convert to array\n",
    "        img_array = cv2.bitwise_not(img_array)\n",
    "        plt.imshow(img_array, cmap='gray')  # graph it\n",
    "        plt.show()  # display!\n",
    "        print(class_num)\n",
    "          # we just want one for now so break\n",
    "        break\n",
    "\n",
    "\n",
    "for category in CATEGORIES:  \n",
    "\n",
    "    path = os.path.join(DATADIR,category)  \n",
    "    class_num = CATEGORIES.index(category)  # get the classification  (0 or a 1). 0=notinsquare 1=insquare\n",
    "\n",
    "    for img in tqdm(os.listdir(path)):  # iterate over each image per point value\n",
    "        img_array = cv2.imread(os.path.join(path,img) ,cv2.IMREAD_GRAYSCALE)  # convert to array\n",
    "        img_array = cv2.bitwise_not(img_array)\n",
    "        new_img_array = rotate(img_array, 90)\n",
    "        new_img_array1 = rotate(img_array, 180)\n",
    "        new_img_array2 = rotate(img_array, 270)\n",
    "\n",
    "        trainys.append(class_num)\n",
    "        trainys.append(class_num)\n",
    "        trainys.append(class_num)\n",
    "        trainys.append(class_num)\n",
    "        trainxs.append(new_img_array)\n",
    "        trainxs.append(new_img_array1)\n",
    "        trainxs.append(new_img_array2)\n",
    "        trainxs.append(img_array)\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6400, 70, 70)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "trainxs1 = np.asarray(trainxs, dtype=np.float32)\n",
    "trainys1 = np.asarray(trainys, dtype=np.float32)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(trainxs1, trainys1, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "print(x_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6400, 70, 70, 1)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
    "#x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
    "input_shape = (img_rows, img_cols, 1)\n",
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yes\n",
      "x_train shape: (6400, 70, 70, 1)\n",
      "x_train shape: (6400, 70, 70, 1)\n",
      "6400 train samples\n",
      "1600 test samples\n",
      "Model: \"sequential_13\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_28 (Conv2D)           (None, 62, 62, 32)        2624      \n",
      "_________________________________________________________________\n",
      "conv2d_29 (Conv2D)           (None, 60, 60, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_13 (MaxPooling (None, 30, 30, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_20 (Dropout)         (None, 30, 30, 64)        0         \n",
      "_________________________________________________________________\n",
      "flatten_12 (Flatten)         (None, 57600)             0         \n",
      "_________________________________________________________________\n",
      "dense_25 (Dense)             (None, 128)               7372928   \n",
      "_________________________________________________________________\n",
      "dropout_21 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_26 (Dense)             (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 7,394,177\n",
      "Trainable params: 7,394,177\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "print(\"yes\")\n",
    "print('x_train shape:', x_train.shape)\n",
    "x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
    "x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
    "input_shape = (img_rows, img_cols, 1)\n",
    "\n",
    "#print(x_train[3])\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "print('x_train shape:', x_train.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, kernel_size=(9, 9),\n",
    "                 activation='relu',\n",
    "                 input_shape=input_shape))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss=keras.losses.binary_crossentropy,\n",
    "              optimizer=keras.optimizers.Adadelta(),\n",
    "              metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6400 samples, validate on 1600 samples\n",
      "Epoch 1/5\n",
      "6400/6400 [==============================] - ETA: 3:01 - loss: 0.6924 - accuracy: 0.54 - ETA: 2:45 - loss: 0.6908 - accuracy: 0.55 - ETA: 2:35 - loss: 0.6898 - accuracy: 0.56 - ETA: 2:29 - loss: 0.6890 - accuracy: 0.55 - ETA: 2:24 - loss: 0.6895 - accuracy: 0.52 - ETA: 2:21 - loss: 0.6886 - accuracy: 0.50 - ETA: 2:17 - loss: 0.6894 - accuracy: 0.49 - ETA: 2:17 - loss: 0.6875 - accuracy: 0.50 - ETA: 2:14 - loss: 0.6820 - accuracy: 0.51 - ETA: 2:11 - loss: 0.6768 - accuracy: 0.52 - ETA: 2:08 - loss: 0.6902 - accuracy: 0.52 - ETA: 2:05 - loss: 0.6900 - accuracy: 0.52 - ETA: 2:04 - loss: 0.6899 - accuracy: 0.51 - ETA: 2:03 - loss: 0.6899 - accuracy: 0.52 - ETA: 2:02 - loss: 0.6896 - accuracy: 0.51 - ETA: 2:00 - loss: 0.6884 - accuracy: 0.51 - ETA: 1:58 - loss: 0.6886 - accuracy: 0.52 - ETA: 1:57 - loss: 0.6884 - accuracy: 0.52 - ETA: 1:55 - loss: 0.6872 - accuracy: 0.52 - ETA: 1:53 - loss: 0.6862 - accuracy: 0.52 - ETA: 1:51 - loss: 0.6846 - accuracy: 0.53 - ETA: 1:49 - loss: 0.6859 - accuracy: 0.52 - ETA: 1:47 - loss: 0.6850 - accuracy: 0.52 - ETA: 1:46 - loss: 0.6855 - accuracy: 0.52 - ETA: 1:45 - loss: 0.6859 - accuracy: 0.52 - ETA: 1:43 - loss: 0.6854 - accuracy: 0.52 - ETA: 1:42 - loss: 0.6846 - accuracy: 0.52 - ETA: 1:40 - loss: 0.6836 - accuracy: 0.52 - ETA: 1:39 - loss: 0.6828 - accuracy: 0.52 - ETA: 1:37 - loss: 0.6818 - accuracy: 0.52 - ETA: 1:36 - loss: 0.6798 - accuracy: 0.52 - ETA: 1:35 - loss: 0.6808 - accuracy: 0.52 - ETA: 1:33 - loss: 0.6797 - accuracy: 0.52 - ETA: 1:31 - loss: 0.6797 - accuracy: 0.52 - ETA: 1:30 - loss: 0.6795 - accuracy: 0.52 - ETA: 1:28 - loss: 0.6791 - accuracy: 0.53 - ETA: 1:27 - loss: 0.6787 - accuracy: 0.53 - ETA: 1:26 - loss: 0.6778 - accuracy: 0.53 - ETA: 1:24 - loss: 0.6772 - accuracy: 0.53 - ETA: 1:23 - loss: 0.6762 - accuracy: 0.52 - ETA: 1:21 - loss: 0.6761 - accuracy: 0.52 - ETA: 1:20 - loss: 0.6752 - accuracy: 0.53 - ETA: 1:18 - loss: 0.6754 - accuracy: 0.53 - ETA: 1:17 - loss: 0.6737 - accuracy: 0.53 - ETA: 1:15 - loss: 0.6735 - accuracy: 0.53 - ETA: 1:14 - loss: 0.6723 - accuracy: 0.53 - ETA: 1:12 - loss: 0.6720 - accuracy: 0.53 - ETA: 1:11 - loss: 0.6717 - accuracy: 0.53 - ETA: 1:10 - loss: 0.6712 - accuracy: 0.53 - ETA: 1:08 - loss: 0.6702 - accuracy: 0.53 - ETA: 1:07 - loss: 0.6692 - accuracy: 0.53 - ETA: 1:06 - loss: 0.6685 - accuracy: 0.53 - ETA: 1:04 - loss: 0.6675 - accuracy: 0.53 - ETA: 1:03 - loss: 0.6671 - accuracy: 0.53 - ETA: 1:01 - loss: 0.6664 - accuracy: 0.53 - ETA: 1:00 - loss: 0.6660 - accuracy: 0.53 - ETA: 58s - loss: 0.6656 - accuracy: 0.5403 - ETA: 57s - loss: 0.6652 - accuracy: 0.539 - ETA: 56s - loss: 0.6647 - accuracy: 0.540 - ETA: 54s - loss: 0.6633 - accuracy: 0.540 - ETA: 53s - loss: 0.6627 - accuracy: 0.541 - ETA: 52s - loss: 0.6617 - accuracy: 0.542 - ETA: 50s - loss: 0.6607 - accuracy: 0.544 - ETA: 49s - loss: 0.6603 - accuracy: 0.544 - ETA: 47s - loss: 0.6604 - accuracy: 0.543 - ETA: 46s - loss: 0.6592 - accuracy: 0.544 - ETA: 45s - loss: 0.6587 - accuracy: 0.544 - ETA: 43s - loss: 0.6577 - accuracy: 0.544 - ETA: 43s - loss: 0.6569 - accuracy: 0.544 - ETA: 42s - loss: 0.6560 - accuracy: 0.544 - ETA: 41s - loss: 0.6551 - accuracy: 0.547 - ETA: 40s - loss: 0.6534 - accuracy: 0.548 - ETA: 39s - loss: 0.6525 - accuracy: 0.549 - ETA: 37s - loss: 0.6515 - accuracy: 0.550 - ETA: 36s - loss: 0.6523 - accuracy: 0.549 - ETA: 34s - loss: 0.6516 - accuracy: 0.550 - ETA: 33s - loss: 0.6508 - accuracy: 0.551 - ETA: 32s - loss: 0.6498 - accuracy: 0.551 - ETA: 30s - loss: 0.6489 - accuracy: 0.552 - ETA: 29s - loss: 0.6475 - accuracy: 0.554 - ETA: 27s - loss: 0.6470 - accuracy: 0.554 - ETA: 26s - loss: 0.6464 - accuracy: 0.554 - ETA: 24s - loss: 0.6455 - accuracy: 0.555 - ETA: 23s - loss: 0.6455 - accuracy: 0.553 - ETA: 21s - loss: 0.6447 - accuracy: 0.553 - ETA: 20s - loss: 0.6433 - accuracy: 0.554 - ETA: 18s - loss: 0.6430 - accuracy: 0.553 - ETA: 17s - loss: 0.6422 - accuracy: 0.554 - ETA: 16s - loss: 0.6413 - accuracy: 0.555 - ETA: 14s - loss: 0.6406 - accuracy: 0.556 - ETA: 13s - loss: 0.6396 - accuracy: 0.557 - ETA: 11s - loss: 0.6382 - accuracy: 0.558 - ETA: 10s - loss: 0.6371 - accuracy: 0.559 - ETA: 8s - loss: 0.6359 - accuracy: 0.561 - ETA: 7s - loss: 0.6357 - accuracy: 0.56 - ETA: 5s - loss: 0.6352 - accuracy: 0.56 - ETA: 4s - loss: 0.6347 - accuracy: 0.56 - ETA: 2s - loss: 0.6333 - accuracy: 0.56 - ETA: 1s - loss: 0.6324 - accuracy: 0.56 - 153s 24ms/step - loss: 0.6320 - accuracy: 0.5652 - val_loss: 0.5493 - val_accuracy: 0.6175\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6400/6400 [==============================] - ETA: 2:11 - loss: 0.5012 - accuracy: 0.68 - ETA: 2:08 - loss: 0.5054 - accuracy: 0.71 - ETA: 2:07 - loss: 0.5010 - accuracy: 0.72 - ETA: 2:05 - loss: 0.5087 - accuracy: 0.70 - ETA: 2:03 - loss: 0.5216 - accuracy: 0.69 - ETA: 2:02 - loss: 0.5188 - accuracy: 0.68 - ETA: 2:02 - loss: 0.5174 - accuracy: 0.66 - ETA: 2:03 - loss: 0.5294 - accuracy: 0.65 - ETA: 2:02 - loss: 0.5722 - accuracy: 0.64 - ETA: 2:01 - loss: 0.5675 - accuracy: 0.64 - ETA: 2:00 - loss: 0.5646 - accuracy: 0.65 - ETA: 1:59 - loss: 0.5617 - accuracy: 0.65 - ETA: 1:57 - loss: 0.5562 - accuracy: 0.65 - ETA: 1:56 - loss: 0.5547 - accuracy: 0.65 - ETA: 1:55 - loss: 0.5552 - accuracy: 0.65 - ETA: 1:53 - loss: 0.5583 - accuracy: 0.65 - ETA: 1:52 - loss: 0.5564 - accuracy: 0.65 - ETA: 1:50 - loss: 0.5553 - accuracy: 0.65 - ETA: 1:49 - loss: 0.5563 - accuracy: 0.64 - ETA: 1:48 - loss: 0.5550 - accuracy: 0.64 - ETA: 1:46 - loss: 0.5537 - accuracy: 0.64 - ETA: 1:45 - loss: 0.5526 - accuracy: 0.63 - ETA: 1:44 - loss: 0.5551 - accuracy: 0.63 - ETA: 1:42 - loss: 0.5573 - accuracy: 0.63 - ETA: 1:41 - loss: 0.5564 - accuracy: 0.62 - ETA: 1:39 - loss: 0.5559 - accuracy: 0.62 - ETA: 1:38 - loss: 0.5559 - accuracy: 0.62 - ETA: 1:36 - loss: 0.5549 - accuracy: 0.62 - ETA: 1:35 - loss: 0.5543 - accuracy: 0.62 - ETA: 1:34 - loss: 0.5532 - accuracy: 0.61 - ETA: 1:32 - loss: 0.5526 - accuracy: 0.61 - ETA: 1:31 - loss: 0.5520 - accuracy: 0.62 - ETA: 1:30 - loss: 0.5517 - accuracy: 0.61 - ETA: 1:29 - loss: 0.5514 - accuracy: 0.61 - ETA: 1:27 - loss: 0.5510 - accuracy: 0.61 - ETA: 1:26 - loss: 0.5498 - accuracy: 0.61 - ETA: 1:24 - loss: 0.5494 - accuracy: 0.61 - ETA: 1:23 - loss: 0.5493 - accuracy: 0.61 - ETA: 1:22 - loss: 0.5482 - accuracy: 0.61 - ETA: 1:20 - loss: 0.5491 - accuracy: 0.61 - ETA: 1:19 - loss: 0.5492 - accuracy: 0.61 - ETA: 1:17 - loss: 0.5495 - accuracy: 0.61 - ETA: 1:16 - loss: 0.5499 - accuracy: 0.62 - ETA: 1:15 - loss: 0.5487 - accuracy: 0.62 - ETA: 1:14 - loss: 0.5479 - accuracy: 0.62 - ETA: 1:12 - loss: 0.5475 - accuracy: 0.61 - ETA: 1:11 - loss: 0.5459 - accuracy: 0.62 - ETA: 1:10 - loss: 0.5446 - accuracy: 0.62 - ETA: 1:08 - loss: 0.5432 - accuracy: 0.62 - ETA: 1:07 - loss: 0.5443 - accuracy: 0.62 - ETA: 1:05 - loss: 0.5423 - accuracy: 0.62 - ETA: 1:04 - loss: 0.5418 - accuracy: 0.62 - ETA: 1:03 - loss: 0.5418 - accuracy: 0.62 - ETA: 1:01 - loss: 0.5408 - accuracy: 0.62 - ETA: 1:00 - loss: 0.5396 - accuracy: 0.62 - ETA: 59s - loss: 0.5397 - accuracy: 0.6236 - ETA: 58s - loss: 0.5397 - accuracy: 0.625 - ETA: 56s - loss: 0.5405 - accuracy: 0.624 - ETA: 55s - loss: 0.5408 - accuracy: 0.622 - ETA: 54s - loss: 0.5412 - accuracy: 0.621 - ETA: 52s - loss: 0.5401 - accuracy: 0.622 - ETA: 51s - loss: 0.5399 - accuracy: 0.621 - ETA: 49s - loss: 0.5382 - accuracy: 0.622 - ETA: 48s - loss: 0.5385 - accuracy: 0.620 - ETA: 47s - loss: 0.5390 - accuracy: 0.620 - ETA: 45s - loss: 0.5385 - accuracy: 0.620 - ETA: 44s - loss: 0.5385 - accuracy: 0.620 - ETA: 43s - loss: 0.5382 - accuracy: 0.620 - ETA: 41s - loss: 0.5376 - accuracy: 0.620 - ETA: 40s - loss: 0.5372 - accuracy: 0.621 - ETA: 39s - loss: 0.5368 - accuracy: 0.622 - ETA: 37s - loss: 0.5363 - accuracy: 0.622 - ETA: 36s - loss: 0.5362 - accuracy: 0.621 - ETA: 35s - loss: 0.5364 - accuracy: 0.621 - ETA: 33s - loss: 0.5369 - accuracy: 0.621 - ETA: 32s - loss: 0.5370 - accuracy: 0.620 - ETA: 31s - loss: 0.5368 - accuracy: 0.620 - ETA: 29s - loss: 0.5364 - accuracy: 0.621 - ETA: 28s - loss: 0.5359 - accuracy: 0.622 - ETA: 27s - loss: 0.5360 - accuracy: 0.621 - ETA: 25s - loss: 0.5359 - accuracy: 0.622 - ETA: 24s - loss: 0.5362 - accuracy: 0.622 - ETA: 23s - loss: 0.5366 - accuracy: 0.621 - ETA: 21s - loss: 0.5362 - accuracy: 0.621 - ETA: 20s - loss: 0.5360 - accuracy: 0.621 - ETA: 18s - loss: 0.5355 - accuracy: 0.622 - ETA: 17s - loss: 0.5361 - accuracy: 0.622 - ETA: 16s - loss: 0.5351 - accuracy: 0.624 - ETA: 14s - loss: 0.5345 - accuracy: 0.624 - ETA: 13s - loss: 0.5352 - accuracy: 0.624 - ETA: 12s - loss: 0.5341 - accuracy: 0.625 - ETA: 10s - loss: 0.5341 - accuracy: 0.624 - ETA: 9s - loss: 0.5346 - accuracy: 0.623 - ETA: 8s - loss: 0.5343 - accuracy: 0.62 - ETA: 6s - loss: 0.5337 - accuracy: 0.62 - ETA: 5s - loss: 0.5334 - accuracy: 0.62 - ETA: 4s - loss: 0.5332 - accuracy: 0.62 - ETA: 2s - loss: 0.5333 - accuracy: 0.62 - ETA: 1s - loss: 0.5328 - accuracy: 0.62 - 143s 22ms/step - loss: 0.5333 - accuracy: 0.6208 - val_loss: 0.5328 - val_accuracy: 0.6137\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6400/6400 [==============================] - ETA: 2:11 - loss: 0.5138 - accuracy: 0.64 - ETA: 2:09 - loss: 0.5278 - accuracy: 0.60 - ETA: 2:08 - loss: 0.5254 - accuracy: 0.63 - ETA: 2:06 - loss: 0.5274 - accuracy: 0.63 - ETA: 2:04 - loss: 0.5149 - accuracy: 0.64 - ETA: 2:03 - loss: 0.5098 - accuracy: 0.65 - ETA: 2:02 - loss: 0.5179 - accuracy: 0.65 - ETA: 2:01 - loss: 0.5162 - accuracy: 0.64 - ETA: 2:01 - loss: 0.5204 - accuracy: 0.63 - ETA: 2:00 - loss: 0.5186 - accuracy: 0.63 - ETA: 1:59 - loss: 0.5179 - accuracy: 0.63 - ETA: 1:58 - loss: 0.5208 - accuracy: 0.63 - ETA: 1:56 - loss: 0.5216 - accuracy: 0.62 - ETA: 1:55 - loss: 0.5252 - accuracy: 0.61 - ETA: 1:53 - loss: 0.5277 - accuracy: 0.61 - ETA: 1:52 - loss: 0.5256 - accuracy: 0.61 - ETA: 1:50 - loss: 0.5241 - accuracy: 0.61 - ETA: 1:49 - loss: 0.5263 - accuracy: 0.61 - ETA: 1:48 - loss: 0.5270 - accuracy: 0.61 - ETA: 1:47 - loss: 0.5253 - accuracy: 0.61 - ETA: 1:46 - loss: 0.5267 - accuracy: 0.61 - ETA: 1:44 - loss: 0.5272 - accuracy: 0.61 - ETA: 1:43 - loss: 0.5254 - accuracy: 0.61 - ETA: 1:42 - loss: 0.5266 - accuracy: 0.61 - ETA: 1:41 - loss: 0.5250 - accuracy: 0.61 - ETA: 1:39 - loss: 0.5236 - accuracy: 0.62 - ETA: 1:38 - loss: 0.5250 - accuracy: 0.61 - ETA: 1:36 - loss: 0.5266 - accuracy: 0.61 - ETA: 1:35 - loss: 0.5250 - accuracy: 0.61 - ETA: 1:34 - loss: 0.5234 - accuracy: 0.61 - ETA: 1:33 - loss: 0.5232 - accuracy: 0.61 - ETA: 1:32 - loss: 0.5232 - accuracy: 0.61 - ETA: 1:30 - loss: 0.5229 - accuracy: 0.61 - ETA: 1:29 - loss: 0.5219 - accuracy: 0.61 - ETA: 1:28 - loss: 0.5202 - accuracy: 0.61 - ETA: 1:27 - loss: 0.5203 - accuracy: 0.61 - ETA: 1:25 - loss: 0.5192 - accuracy: 0.61 - ETA: 1:24 - loss: 0.5186 - accuracy: 0.62 - ETA: 1:22 - loss: 0.5190 - accuracy: 0.62 - ETA: 1:21 - loss: 0.5172 - accuracy: 0.62 - ETA: 1:20 - loss: 0.5171 - accuracy: 0.62 - ETA: 1:18 - loss: 0.5168 - accuracy: 0.62 - ETA: 1:17 - loss: 0.5164 - accuracy: 0.62 - ETA: 1:16 - loss: 0.5148 - accuracy: 0.62 - ETA: 1:14 - loss: 0.5158 - accuracy: 0.62 - ETA: 1:13 - loss: 0.5161 - accuracy: 0.62 - ETA: 1:12 - loss: 0.5160 - accuracy: 0.62 - ETA: 1:10 - loss: 0.5165 - accuracy: 0.62 - ETA: 1:09 - loss: 0.5159 - accuracy: 0.62 - ETA: 1:07 - loss: 0.5161 - accuracy: 0.62 - ETA: 1:06 - loss: 0.5156 - accuracy: 0.62 - ETA: 1:05 - loss: 0.5159 - accuracy: 0.62 - ETA: 1:03 - loss: 0.5168 - accuracy: 0.62 - ETA: 1:02 - loss: 0.5167 - accuracy: 0.62 - ETA: 1:00 - loss: 0.5157 - accuracy: 0.62 - ETA: 59s - loss: 0.5171 - accuracy: 0.6261 - ETA: 58s - loss: 0.5172 - accuracy: 0.626 - ETA: 57s - loss: 0.5173 - accuracy: 0.626 - ETA: 55s - loss: 0.5180 - accuracy: 0.626 - ETA: 54s - loss: 0.5174 - accuracy: 0.625 - ETA: 52s - loss: 0.5181 - accuracy: 0.625 - ETA: 51s - loss: 0.5180 - accuracy: 0.624 - ETA: 50s - loss: 0.5170 - accuracy: 0.625 - ETA: 48s - loss: 0.5176 - accuracy: 0.626 - ETA: 47s - loss: 0.5172 - accuracy: 0.625 - ETA: 46s - loss: 0.5187 - accuracy: 0.625 - ETA: 44s - loss: 0.5191 - accuracy: 0.625 - ETA: 43s - loss: 0.5190 - accuracy: 0.625 - ETA: 42s - loss: 0.5193 - accuracy: 0.624 - ETA: 40s - loss: 0.5198 - accuracy: 0.623 - ETA: 39s - loss: 0.5206 - accuracy: 0.623 - ETA: 38s - loss: 0.5201 - accuracy: 0.625 - ETA: 36s - loss: 0.5205 - accuracy: 0.625 - ETA: 35s - loss: 0.5205 - accuracy: 0.624 - ETA: 33s - loss: 0.5191 - accuracy: 0.625 - ETA: 32s - loss: 0.5195 - accuracy: 0.623 - ETA: 31s - loss: 0.5199 - accuracy: 0.623 - ETA: 29s - loss: 0.5205 - accuracy: 0.623 - ETA: 28s - loss: 0.5198 - accuracy: 0.623 - ETA: 27s - loss: 0.5194 - accuracy: 0.623 - ETA: 25s - loss: 0.5196 - accuracy: 0.623 - ETA: 24s - loss: 0.5196 - accuracy: 0.624 - ETA: 23s - loss: 0.5195 - accuracy: 0.624 - ETA: 21s - loss: 0.5195 - accuracy: 0.623 - ETA: 20s - loss: 0.5206 - accuracy: 0.624 - ETA: 19s - loss: 0.5211 - accuracy: 0.624 - ETA: 17s - loss: 0.5215 - accuracy: 0.622 - ETA: 16s - loss: 0.5216 - accuracy: 0.622 - ETA: 14s - loss: 0.5207 - accuracy: 0.623 - ETA: 13s - loss: 0.5209 - accuracy: 0.622 - ETA: 12s - loss: 0.5212 - accuracy: 0.622 - ETA: 10s - loss: 0.5208 - accuracy: 0.621 - ETA: 9s - loss: 0.5207 - accuracy: 0.623 - ETA: 8s - loss: 0.5206 - accuracy: 0.62 - ETA: 6s - loss: 0.5206 - accuracy: 0.62 - ETA: 5s - loss: 0.5210 - accuracy: 0.62 - ETA: 4s - loss: 0.5209 - accuracy: 0.62 - ETA: 2s - loss: 0.5205 - accuracy: 0.62 - ETA: 1s - loss: 0.5205 - accuracy: 0.62 - 144s 23ms/step - loss: 0.5201 - accuracy: 0.6236 - val_loss: 0.5319 - val_accuracy: 0.6194\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6400/6400 [==============================] - ETA: 2:10 - loss: 0.4707 - accuracy: 0.60 - ETA: 2:08 - loss: 0.4897 - accuracy: 0.64 - ETA: 2:07 - loss: 0.4924 - accuracy: 0.65 - ETA: 2:06 - loss: 0.4889 - accuracy: 0.64 - ETA: 2:04 - loss: 0.4907 - accuracy: 0.64 - ETA: 2:03 - loss: 0.4955 - accuracy: 0.64 - ETA: 2:02 - loss: 0.5031 - accuracy: 0.62 - ETA: 2:01 - loss: 0.5068 - accuracy: 0.63 - ETA: 2:01 - loss: 0.5012 - accuracy: 0.65 - ETA: 2:00 - loss: 0.5033 - accuracy: 0.65 - ETA: 1:59 - loss: 0.5049 - accuracy: 0.65 - ETA: 1:58 - loss: 0.5062 - accuracy: 0.64 - ETA: 1:56 - loss: 0.5047 - accuracy: 0.64 - ETA: 1:55 - loss: 0.5058 - accuracy: 0.64 - ETA: 1:53 - loss: 0.5068 - accuracy: 0.64 - ETA: 1:52 - loss: 0.5116 - accuracy: 0.64 - ETA: 1:50 - loss: 0.5129 - accuracy: 0.64 - ETA: 1:49 - loss: 0.5121 - accuracy: 0.64 - ETA: 1:48 - loss: 0.5142 - accuracy: 0.64 - ETA: 1:47 - loss: 0.5155 - accuracy: 0.63 - ETA: 1:46 - loss: 0.5147 - accuracy: 0.64 - ETA: 1:45 - loss: 0.5141 - accuracy: 0.63 - ETA: 1:43 - loss: 0.5159 - accuracy: 0.63 - ETA: 1:42 - loss: 0.5168 - accuracy: 0.63 - ETA: 1:40 - loss: 0.5157 - accuracy: 0.63 - ETA: 1:39 - loss: 0.5131 - accuracy: 0.63 - ETA: 1:38 - loss: 0.5138 - accuracy: 0.63 - ETA: 1:38 - loss: 0.5137 - accuracy: 0.63 - ETA: 1:38 - loss: 0.5135 - accuracy: 0.63 - ETA: 1:37 - loss: 0.5134 - accuracy: 0.63 - ETA: 1:36 - loss: 0.5146 - accuracy: 0.63 - ETA: 1:35 - loss: 0.5138 - accuracy: 0.63 - ETA: 1:33 - loss: 0.5128 - accuracy: 0.63 - ETA: 1:33 - loss: 0.5128 - accuracy: 0.63 - ETA: 1:31 - loss: 0.5109 - accuracy: 0.63 - ETA: 1:30 - loss: 0.5100 - accuracy: 0.63 - ETA: 1:28 - loss: 0.5093 - accuracy: 0.63 - ETA: 1:27 - loss: 0.5087 - accuracy: 0.63 - ETA: 1:25 - loss: 0.5081 - accuracy: 0.63 - ETA: 1:23 - loss: 0.5092 - accuracy: 0.63 - ETA: 1:22 - loss: 0.5108 - accuracy: 0.63 - ETA: 1:21 - loss: 0.5118 - accuracy: 0.63 - ETA: 1:19 - loss: 0.5101 - accuracy: 0.63 - ETA: 1:18 - loss: 0.5104 - accuracy: 0.64 - ETA: 1:17 - loss: 0.5110 - accuracy: 0.63 - ETA: 1:15 - loss: 0.5108 - accuracy: 0.63 - ETA: 1:14 - loss: 0.5101 - accuracy: 0.64 - ETA: 1:12 - loss: 0.5099 - accuracy: 0.63 - ETA: 1:11 - loss: 0.5101 - accuracy: 0.63 - ETA: 1:09 - loss: 0.5105 - accuracy: 0.63 - ETA: 1:08 - loss: 0.5107 - accuracy: 0.63 - ETA: 1:06 - loss: 0.5096 - accuracy: 0.63 - ETA: 1:05 - loss: 0.5109 - accuracy: 0.63 - ETA: 1:04 - loss: 0.5100 - accuracy: 0.64 - ETA: 1:02 - loss: 0.5107 - accuracy: 0.63 - ETA: 1:01 - loss: 0.5115 - accuracy: 0.63 - ETA: 59s - loss: 0.5103 - accuracy: 0.6368 - ETA: 58s - loss: 0.5106 - accuracy: 0.637 - ETA: 56s - loss: 0.5115 - accuracy: 0.639 - ETA: 55s - loss: 0.5125 - accuracy: 0.639 - ETA: 54s - loss: 0.5135 - accuracy: 0.638 - ETA: 52s - loss: 0.5140 - accuracy: 0.637 - ETA: 51s - loss: 0.5138 - accuracy: 0.637 - ETA: 49s - loss: 0.5144 - accuracy: 0.636 - ETA: 48s - loss: 0.5148 - accuracy: 0.635 - ETA: 47s - loss: 0.5152 - accuracy: 0.636 - ETA: 45s - loss: 0.5156 - accuracy: 0.636 - ETA: 44s - loss: 0.5157 - accuracy: 0.636 - ETA: 42s - loss: 0.5157 - accuracy: 0.635 - ETA: 41s - loss: 0.5157 - accuracy: 0.635 - ETA: 40s - loss: 0.5162 - accuracy: 0.634 - ETA: 38s - loss: 0.5169 - accuracy: 0.633 - ETA: 37s - loss: 0.5171 - accuracy: 0.633 - ETA: 35s - loss: 0.5173 - accuracy: 0.631 - ETA: 34s - loss: 0.5165 - accuracy: 0.631 - ETA: 33s - loss: 0.5166 - accuracy: 0.630 - ETA: 31s - loss: 0.5164 - accuracy: 0.629 - ETA: 30s - loss: 0.5162 - accuracy: 0.629 - ETA: 28s - loss: 0.5165 - accuracy: 0.627 - ETA: 27s - loss: 0.5173 - accuracy: 0.626 - ETA: 26s - loss: 0.5164 - accuracy: 0.627 - ETA: 24s - loss: 0.5168 - accuracy: 0.627 - ETA: 23s - loss: 0.5167 - accuracy: 0.627 - ETA: 22s - loss: 0.5171 - accuracy: 0.627 - ETA: 20s - loss: 0.5167 - accuracy: 0.627 - ETA: 19s - loss: 0.5168 - accuracy: 0.627 - ETA: 17s - loss: 0.5175 - accuracy: 0.626 - ETA: 16s - loss: 0.5171 - accuracy: 0.627 - ETA: 15s - loss: 0.5177 - accuracy: 0.626 - ETA: 13s - loss: 0.5181 - accuracy: 0.626 - ETA: 12s - loss: 0.5177 - accuracy: 0.626 - ETA: 11s - loss: 0.5182 - accuracy: 0.625 - ETA: 9s - loss: 0.5181 - accuracy: 0.625 - ETA: 8s - loss: 0.5184 - accuracy: 0.62 - ETA: 6s - loss: 0.5189 - accuracy: 0.62 - ETA: 5s - loss: 0.5188 - accuracy: 0.62 - ETA: 4s - loss: 0.5179 - accuracy: 0.62 - ETA: 2s - loss: 0.5177 - accuracy: 0.62 - ETA: 1s - loss: 0.5178 - accuracy: 0.62 - 151s 24ms/step - loss: 0.5179 - accuracy: 0.6256 - val_loss: 0.5317 - val_accuracy: 0.6137\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6400/6400 [==============================] - ETA: 2:29 - loss: 0.5190 - accuracy: 0.60 - ETA: 2:37 - loss: 0.5150 - accuracy: 0.64 - ETA: 2:42 - loss: 0.5348 - accuracy: 0.62 - ETA: 2:47 - loss: 0.5229 - accuracy: 0.62 - ETA: 2:42 - loss: 0.5201 - accuracy: 0.61 - ETA: 2:36 - loss: 0.5133 - accuracy: 0.61 - ETA: 2:30 - loss: 0.5158 - accuracy: 0.61 - ETA: 2:25 - loss: 0.5135 - accuracy: 0.62 - ETA: 2:22 - loss: 0.5249 - accuracy: 0.62 - ETA: 2:19 - loss: 0.5255 - accuracy: 0.62 - ETA: 2:18 - loss: 0.5243 - accuracy: 0.62 - ETA: 2:16 - loss: 0.5221 - accuracy: 0.62 - ETA: 2:14 - loss: 0.5221 - accuracy: 0.62 - ETA: 2:12 - loss: 0.5244 - accuracy: 0.61 - ETA: 2:09 - loss: 0.5263 - accuracy: 0.61 - ETA: 2:07 - loss: 0.5288 - accuracy: 0.60 - ETA: 2:06 - loss: 0.5315 - accuracy: 0.60 - ETA: 2:04 - loss: 0.5344 - accuracy: 0.60 - ETA: 2:02 - loss: 0.5326 - accuracy: 0.60 - ETA: 2:00 - loss: 0.5333 - accuracy: 0.60 - ETA: 1:58 - loss: 0.5306 - accuracy: 0.60 - ETA: 1:56 - loss: 0.5306 - accuracy: 0.60 - ETA: 1:55 - loss: 0.5274 - accuracy: 0.60 - ETA: 1:54 - loss: 0.5266 - accuracy: 0.60 - ETA: 1:52 - loss: 0.5246 - accuracy: 0.61 - ETA: 1:51 - loss: 0.5241 - accuracy: 0.61 - ETA: 1:52 - loss: 0.5232 - accuracy: 0.61 - ETA: 1:51 - loss: 0.5226 - accuracy: 0.61 - ETA: 1:49 - loss: 0.5217 - accuracy: 0.61 - ETA: 1:47 - loss: 0.5216 - accuracy: 0.61 - ETA: 1:46 - loss: 0.5193 - accuracy: 0.62 - ETA: 1:46 - loss: 0.5182 - accuracy: 0.62 - ETA: 1:45 - loss: 0.5159 - accuracy: 0.62 - ETA: 1:44 - loss: 0.5157 - accuracy: 0.63 - ETA: 1:42 - loss: 0.5167 - accuracy: 0.62 - ETA: 1:41 - loss: 0.5179 - accuracy: 0.62 - ETA: 1:39 - loss: 0.5167 - accuracy: 0.62 - ETA: 1:38 - loss: 0.5171 - accuracy: 0.62 - ETA: 1:37 - loss: 0.5165 - accuracy: 0.62 - ETA: 1:36 - loss: 0.5169 - accuracy: 0.62 - ETA: 1:34 - loss: 0.5163 - accuracy: 0.62 - ETA: 1:33 - loss: 0.5159 - accuracy: 0.62 - ETA: 1:31 - loss: 0.5155 - accuracy: 0.62 - ETA: 1:30 - loss: 0.5159 - accuracy: 0.62 - ETA: 1:29 - loss: 0.5165 - accuracy: 0.62 - ETA: 1:28 - loss: 0.5159 - accuracy: 0.62 - ETA: 1:26 - loss: 0.5166 - accuracy: 0.62 - ETA: 1:24 - loss: 0.5176 - accuracy: 0.62 - ETA: 1:23 - loss: 0.5183 - accuracy: 0.62 - ETA: 1:21 - loss: 0.5172 - accuracy: 0.62 - ETA: 1:20 - loss: 0.5177 - accuracy: 0.62 - ETA: 1:18 - loss: 0.5181 - accuracy: 0.63 - ETA: 1:17 - loss: 0.5183 - accuracy: 0.63 - ETA: 1:15 - loss: 0.5189 - accuracy: 0.63 - ETA: 1:14 - loss: 0.5187 - accuracy: 0.63 - ETA: 1:12 - loss: 0.5185 - accuracy: 0.63 - ETA: 1:11 - loss: 0.5188 - accuracy: 0.62 - ETA: 1:09 - loss: 0.5189 - accuracy: 0.62 - ETA: 1:08 - loss: 0.5185 - accuracy: 0.62 - ETA: 1:06 - loss: 0.5184 - accuracy: 0.62 - ETA: 1:04 - loss: 0.5179 - accuracy: 0.62 - ETA: 1:03 - loss: 0.5171 - accuracy: 0.63 - ETA: 1:01 - loss: 0.5163 - accuracy: 0.63 - ETA: 59s - loss: 0.5164 - accuracy: 0.6296 - ETA: 57s - loss: 0.5162 - accuracy: 0.630 - ETA: 56s - loss: 0.5172 - accuracy: 0.631 - ETA: 54s - loss: 0.5181 - accuracy: 0.631 - ETA: 52s - loss: 0.5183 - accuracy: 0.631 - ETA: 51s - loss: 0.5178 - accuracy: 0.632 - ETA: 49s - loss: 0.5183 - accuracy: 0.633 - ETA: 47s - loss: 0.5192 - accuracy: 0.632 - ETA: 45s - loss: 0.5191 - accuracy: 0.632 - ETA: 44s - loss: 0.5190 - accuracy: 0.632 - ETA: 42s - loss: 0.5192 - accuracy: 0.631 - ETA: 40s - loss: 0.5189 - accuracy: 0.631 - ETA: 39s - loss: 0.5189 - accuracy: 0.631 - ETA: 37s - loss: 0.5183 - accuracy: 0.630 - ETA: 35s - loss: 0.5181 - accuracy: 0.629 - ETA: 34s - loss: 0.5182 - accuracy: 0.628 - ETA: 32s - loss: 0.5183 - accuracy: 0.628 - ETA: 30s - loss: 0.5177 - accuracy: 0.628 - ETA: 29s - loss: 0.5174 - accuracy: 0.627 - ETA: 27s - loss: 0.5172 - accuracy: 0.627 - ETA: 25s - loss: 0.5171 - accuracy: 0.628 - ETA: 24s - loss: 0.5168 - accuracy: 0.628 - ETA: 22s - loss: 0.5173 - accuracy: 0.629 - ETA: 20s - loss: 0.5175 - accuracy: 0.629 - ETA: 19s - loss: 0.5180 - accuracy: 0.629 - ETA: 17s - loss: 0.5182 - accuracy: 0.630 - ETA: 15s - loss: 0.5181 - accuracy: 0.630 - ETA: 14s - loss: 0.5180 - accuracy: 0.630 - ETA: 12s - loss: 0.5178 - accuracy: 0.630 - ETA: 11s - loss: 0.5178 - accuracy: 0.631 - ETA: 9s - loss: 0.5174 - accuracy: 0.631 - ETA: 7s - loss: 0.5171 - accuracy: 0.63 - ETA: 6s - loss: 0.5178 - accuracy: 0.62 - ETA: 4s - loss: 0.5179 - accuracy: 0.62 - ETA: 3s - loss: 0.5181 - accuracy: 0.62 - ETA: 1s - loss: 0.5181 - accuracy: 0.62 - 166s 26ms/step - loss: 0.5175 - accuracy: 0.6280 - val_loss: 0.5316 - val_accuracy: 0.6194\n",
      "Test loss: 0.5316305440664292\n",
      "Test accuracy: 0.6193749904632568\n"
     ]
    }
   ],
   "source": [
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          validation_data=(x_test, y_test))\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yes\n",
      "x_train shape: (6400, 70, 70, 1)\n",
      "x_train shape: (6400, 70, 70, 1)\n",
      "6400 train samples\n",
      "1600 test samples\n"
     ]
    }
   ],
   "source": [
    "print(\"yes\")\n",
    "print('x_train shape:', x_train.shape)\n",
    "x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
    "x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
    "input_shape = (img_rows, img_cols, 1)\n",
    "\n",
    "#print(x_train[3])\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "print('x_train shape:', x_train.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Flatten())\n",
    "model.add(Dense(4900, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss=keras.losses.binary_crossentropy,\n",
    "              optimizer=keras.optimizers.Adadelta(),\n",
    "              metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yes\n",
      "x_train shape: (6400, 70, 70, 1)\n",
      "x_train shape: (6400, 70, 70, 1)\n",
      "6400 train samples\n",
      "1600 test samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\colly\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:23: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3), activation=\"relu\")`\n",
      "C:\\Users\\colly\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:25: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3), activation=\"relu\")`\n",
      "C:\\Users\\colly\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:29: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (3, 3), activation=\"relu\")`\n",
      "C:\\Users\\colly\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:31: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (3, 3), activation=\"relu\")`\n",
      "C:\\Users\\colly\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:35: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(256, (3, 3), activation=\"relu\")`\n",
      "C:\\Users\\colly\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:37: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(256, (3, 3), activation=\"relu\")`\n",
      "C:\\Users\\colly\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:39: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(256, (3, 3), activation=\"relu\")`\n",
      "C:\\Users\\colly\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:43: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(512, (3, 3), activation=\"relu\")`\n",
      "C:\\Users\\colly\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:45: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(512, (3, 3), activation=\"relu\")`\n",
      "C:\\Users\\colly\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:47: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(512, (3, 3), activation=\"relu\")`\n",
      "C:\\Users\\colly\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:51: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(512, (3, 3), activation=\"relu\")`\n",
      "C:\\Users\\colly\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:53: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(512, (3, 3), activation=\"relu\")`\n",
      "C:\\Users\\colly\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:55: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(512, (3, 3), activation=\"relu\")`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "zero_padding2d_1 (ZeroPaddin (None, 72, 72, 1)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_11 (Conv2D)           (None, 70, 70, 64)        640       \n",
      "_________________________________________________________________\n",
      "zero_padding2d_2 (ZeroPaddin (None, 72, 72, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_12 (Conv2D)           (None, 70, 70, 64)        36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2 (None, 35, 35, 64)        0         \n",
      "_________________________________________________________________\n",
      "zero_padding2d_3 (ZeroPaddin (None, 37, 37, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_13 (Conv2D)           (None, 35, 35, 128)       73856     \n",
      "_________________________________________________________________\n",
      "zero_padding2d_4 (ZeroPaddin (None, 37, 37, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_14 (Conv2D)           (None, 35, 35, 128)       147584    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_7 (MaxPooling2 (None, 17, 17, 128)       0         \n",
      "_________________________________________________________________\n",
      "zero_padding2d_5 (ZeroPaddin (None, 19, 19, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_15 (Conv2D)           (None, 17, 17, 256)       295168    \n",
      "_________________________________________________________________\n",
      "zero_padding2d_6 (ZeroPaddin (None, 19, 19, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_16 (Conv2D)           (None, 17, 17, 256)       590080    \n",
      "_________________________________________________________________\n",
      "zero_padding2d_7 (ZeroPaddin (None, 19, 19, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_17 (Conv2D)           (None, 17, 17, 256)       590080    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_8 (MaxPooling2 (None, 8, 8, 256)         0         \n",
      "_________________________________________________________________\n",
      "zero_padding2d_8 (ZeroPaddin (None, 10, 10, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_18 (Conv2D)           (None, 8, 8, 512)         1180160   \n",
      "_________________________________________________________________\n",
      "zero_padding2d_9 (ZeroPaddin (None, 10, 10, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_19 (Conv2D)           (None, 8, 8, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "zero_padding2d_10 (ZeroPaddi (None, 10, 10, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_20 (Conv2D)           (None, 8, 8, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "max_pooling2d_9 (MaxPooling2 (None, 4, 4, 512)         0         \n",
      "_________________________________________________________________\n",
      "zero_padding2d_11 (ZeroPaddi (None, 6, 6, 512)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_21 (Conv2D)           (None, 4, 4, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "zero_padding2d_12 (ZeroPaddi (None, 6, 6, 512)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_22 (Conv2D)           (None, 4, 4, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "zero_padding2d_13 (ZeroPaddi (None, 6, 6, 512)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_23 (Conv2D)           (None, 4, 4, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "max_pooling2d_10 (MaxPooling (None, 2, 2, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten_6 (Flatten)          (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 4096)              8392704   \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 4096)              16781312  \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 1)                 4097      \n",
      "=================================================================\n",
      "Total params: 39,891,649\n",
      "Trainable params: 39,891,649\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers.core import Flatten, Dense, Dropout\n",
    "from keras.layers.convolutional import Convolution2D, MaxPooling2D, ZeroPadding2D\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "print(\"yes\")\n",
    "print('x_train shape:', x_train.shape)\n",
    "x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
    "x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
    "input_shape = (img_rows, img_cols, 1)\n",
    "\n",
    "#print(x_train[3])\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "print('x_train shape:', x_train.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "\n",
    "model = Sequential()\n",
    "model.add(ZeroPadding2D((1,1),input_shape=(70,70,1)))\n",
    "model.add(Convolution2D(64, 3, 3, activation='relu'))\n",
    "model.add(ZeroPadding2D((1,1)))\n",
    "model.add(Convolution2D(64, 3, 3, activation='relu'))\n",
    "model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    "\n",
    "model.add(ZeroPadding2D((1,1)))\n",
    "model.add(Convolution2D(128, 3, 3, activation='relu'))\n",
    "model.add(ZeroPadding2D((1,1)))\n",
    "model.add(Convolution2D(128, 3, 3, activation='relu'))\n",
    "model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    "\n",
    "model.add(ZeroPadding2D((1,1)))\n",
    "model.add(Convolution2D(256, 3, 3, activation='relu'))\n",
    "model.add(ZeroPadding2D((1,1)))\n",
    "model.add(Convolution2D(256, 3, 3, activation='relu'))\n",
    "model.add(ZeroPadding2D((1,1)))\n",
    "model.add(Convolution2D(256, 3, 3, activation='relu'))\n",
    "model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    "\n",
    "model.add(ZeroPadding2D((1,1)))\n",
    "model.add(Convolution2D(512, 3, 3, activation='relu'))\n",
    "model.add(ZeroPadding2D((1,1)))\n",
    "model.add(Convolution2D(512, 3, 3, activation='relu'))\n",
    "model.add(ZeroPadding2D((1,1)))\n",
    "model.add(Convolution2D(512, 3, 3, activation='relu'))\n",
    "model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    "\n",
    "model.add(ZeroPadding2D((1,1)))\n",
    "model.add(Convolution2D(512, 3, 3, activation='relu'))\n",
    "model.add(ZeroPadding2D((1,1)))\n",
    "model.add(Convolution2D(512, 3, 3, activation='relu'))\n",
    "model.add(ZeroPadding2D((1,1)))\n",
    "model.add(Convolution2D(512, 3, 3, activation='relu'))\n",
    "model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(4096, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(4096, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "    \n",
    "model.compile(loss=keras.losses.binary_crossentropy,\n",
    "              optimizer=keras.optimizers.Adadelta(),\n",
    "              metrics=['accuracy'])\n",
    "model.summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
